\subsubsection{$L_2$ Feature Regularization Makes Optimization Easier}
Moreover, we analyze numerically how the $L_2$ feature penalty influences the optimization process in our regression problem. % Here, we analyze both the \textit{\textbf{uniform}} and \textit{\textbf{weighted}} $L_2$ feature penalty scenario.

We study a special case $\{\mathbf{x}\in\mathbb{R}^d,y\in\mathbb{R}\}$ with $W_1\in\mathbb{R}^{1\times m}$, $W_2\in\mathbb{R}$ and include offsets $b_1\in\mathbb{R}$ and $b_2\in\mathbb{R}$ to make the problem more general. Then, the latent representation becomes $h=W_1\mathbf{x}+\mathbf{b}_1$ and the prediction is $\hat{y}=W_2h+b_2$. The cost function of Eqn(\ref{eqn:final-cost}) becomes:
%By dropping $\alpha_i$ in Eqn (3), we show that $L_2$ feature penalty actually regularizes the representation similar to a soft batch normalization. Here, we analyze the numerical behavior with $\alpha_i$ remaining.
\begin{equation}
E(W_1,b_1,W_2,b_2)=\frac{1}{2}\mathbb{E}\{(W_2h+b_2-y)^2+\frac{\lambda_1}{2} r_{\dagger}^2h^2\}+\frac{\lambda_2}{2}W_2^2
\label{eqn:final-regression}
\end{equation}
We define the prediction residual $r$ and $r_{\dagger}$ as $\hat{y}-y$ for better readability, and substitute $\alpha^i=(r^i_{\dagger})^2$. 
%to avoid the degenerate case with very $\{\mathbf{w}_1,b_1\}\rightarrow\mathbf{0}$ and $w_2\rightarrow\infty$.
Numerically, we apply a \textbf{two-step} process: in the first step, we calculate the sample-dependent $r^i_{\dagger}$ in the feed-forward process to obtain our $L_2$ feature regularization weights $\alpha^i$ for each $(\mathbf{x}^i,y^i)$; in the second step, we treat $r_{\dagger}$ as a constant in the  optimize. 
%Then in the setting of online learning, 
The gradient and Hessian matrix of Eqn(\ref{eqn:final-regression}) can be derived as:
\begin{align*}
\frac{\partial E}{\partial W_2}&=\mathbb{E}\{rh^T\}+\lambda_2W_2 
&\frac{\partial E}{\partial b_2}&=\mathbb{E}\{r\}\\
\frac{\partial E}{\partial h^i}&=W_2r^i+\lambda_1(r^i_{\dagger})^2h^i\\
\frac{\partial E}{\partial W_1}&=W_2\mathbb{E}\{r\mathbf{x}^T\}+\lambda_1 \mathbb{E}\{r_{\dagger}^2h\mathbf{x}^T\}\\
\frac{\partial E}{\partial b_1}&=W_2\mathbb{E}\{r\}+\lambda_1 \mathbb{E}\{r_{\dagger}^2h\}
\end{align*}

\begin{comment}
\begin{align*}
\frac{\partial E}{\partial W_2}&=\mathbb{E}\{rh^T\}+\lambda_2W_2 
&\frac{\partial E}{\partial b_2}&=\mathbb{E}\{r\}
&\frac{\partial E}{\partial h^i}&=W_2r^i+\lambda_1(r^i_{\dagger})^2h^i\\
\frac{\partial E}{\partial W_1}&=W_2\mathbb{E}\{r\mathbf{x}^T\}+\lambda_1 \mathbb{E}\{r_{\dagger}^2h\mathbf{x}^T\}
&\frac{\partial E}{\partial b_1}&=W_2\mathbb{E}\{r\}+\lambda_1 \mathbb{E}\{r_{\dagger}^2h\}
\end{align*}
\end{comment}
and
\begin{align*}
\frac{\partial^2 E}{\partial W_2^2}&=\mathbb{E}\{hh^T\}+\lambda_2
&\frac{\partial^2 E}{\partial b_2^2}&=1\\
\frac{\partial^2 E}{\partial (h^i)^2}&=W_2^2+\lambda_1 (r^i_{\dagger})^2
&\frac{\partial^2 E}{\partial W_1^2}&=(W_2^2+\lambda_1 		\mathbb{E}\{r_{\dagger}^2\})\mathbb{E}\{\mathbf{x}\mathbf{x}^T\}\\
\frac{\partial^2 E}{\partial b_1^2}&=W_2^2+\lambda_1 \mathbb{E}\{r_{\dagger}^2\}
\end{align*}
Suppose that we apply a second-order optimization algorithm for the network parameter $\theta\in\{w_1,w_2,b_1,b_2\}$, the updates should be $\vartriangle\theta=-\eta*(\partial^2 E/\partial\theta^2)^{-1}(\partial E/\partial\theta)$ with $\eta>0$ as the step-size.
If we unluckily start from a bad initialization point $W_2\rightarrow0$, the updates of $h_i$, $W_1$ and $b_1$ are of the form:
\begin{eqnarray*}
\vartriangle h^i&=&-\eta(W_2^2+\lambda_1 (r^i_{\dagger})^2\})^{-1}(W_2r^i+\lambda_1(r^i_{\dagger})^2h^i)\\
	\vartriangle\mathbf{w}_1&=&-\eta[(W_2^2+\lambda_1\mathbb{E}\{ r_{\dagger}^2\})\mathbb{E}\{\mathbf{x}\mathbf{x}^T\}]^{-1}\\
	&&(W_2\mathbb{E}\{r\}+\lambda_1\mathbb{E}\{r_{\dagger}^2h\})\mathbb{E}\{\mathbf{x}\}\\
	\vartriangle b_1&=&-\eta(W_2^2+\lambda_1 \mathbb{E}\{r_{\dagger}^2\})^{-1}(W_2\mathbb{E}\{r\}+\lambda_1 \mathbb{E}\{r_{\dagger}^2h\})
\end{eqnarray*}
The updates will become very ill-conditioned without the regularization term ($\lambda_1=0$), since spectrum of the Hessian matrix is two-orders of infinitesimal $O(W_2^2)$ and the gradient is of one-order $O(W_2)$. In comparison, with a reasonable choice of $\lambda_1>0$, the computation can be \textbf{largely stabilized} when $\mathbb{E}\{r^2_\dagger\}\neq0$.

When the algorithm finally converges to a local minimum $\{W_1^*,b_1^*,W_2^*,b_2^*\}$, the expectation of parameter and latent feature should have gradient close to $\mathbf{0}$:
\begin{align*}
\frac{\partial E}{\partial W_2}&=\mathbb{E}\{rh^T\}+\lambda_2W_2\rightarrow0 &
\frac{\partial E}{\partial b_2}&=\mathbb{E}\{r\}\rightarrow0
\end{align*}
Substituting this in the analysis of $b_1$, we have:
\begin{eqnarray}
%\frac{\partial E}{\partial h_i}\approx W_2 r_i+\lambda \alpha h_i\approx 0&\Longrightarrow& \mathbb{E}\{\alpha h\}\approx0\\
\frac{\partial E}{\partial b_1}=W_2\mathbb{E}\{r\}+\lambda_1 \mathbb{E}\{\alpha h\}&\Longrightarrow& \mathbb{E}\{\alpha h\}\rightarrow0
\label{eqn:center}
\end{eqnarray}
In other words, the feature penalty \textit{\textbf{centralizes}} the {\color{red}final hidden layer} representation $h(\mathbf{x})=\phi(\mathbf{x})$. Especially, in the \textbf{uniform $L_2$-feature penalty} case, we simply drop $\alpha$ in Eqn (\ref{eqn:center}) and have $\mathbb{E}\{h\}=\mathbb{E}\{\phi(\mathbf{x})\}\rightarrow0$. 

In summary, the feature penalty \textit{\textbf{improves the numerical stability}} of the optimization process in the regression problem. The conclusion also holds for the classification.
%Furthermore, the centralization of $h=\phi(x)$ in turn makes the regression learning $W_2$ easier as well. : in case of a high-dimensional $\mathbf{h}$, the Hessian matrix $\frac{\partial^2E}{\partial\mathbf{w}_2^2}=\mathbb{E}\{\mathbf{h}\mathbf{h}^T\}+\lambda_2\mathbf{I}$ is better conditioned when $\mathbf{h}$ is centered and such whitening speeds up convergence.

{\color{red}Similar results for $\phi(\mathbf{x})$ can be extended to deeper multilayer perceptrons with convex differentiable non-linear activation functions such as ReLU and max-pooling. In an $m$-layer model parametrized by $\{W_1,W_2,...,W_m\}$, the Hessian matrix of hidden layers becomes strongly convex by back-propagating from the regularizer $||\sigma_{m-1}(W_{m-1}*\sigma_{m-2}(W_2*(...*\sigma_{1}(W_1\mathbf{x}))))||^2$.} % such as the soft rectified linear units $h=1+\exp(\mathbf{w}^T_1\mathbf{x})$.