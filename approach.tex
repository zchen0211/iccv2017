\section{An Analysis of Feature Regularization}
We briefly introduce the notations in our work: we denote uppercase $A$, bold $\mathbf{a}$ and lowercase $a$ for matrices, vectors and scalars respectively. For a vector $\mathbf{a}_i$, we denotes $\mathbf{a}_{i,j} $ as its $j$-th element.  $||.||_F$ stands for the Frobenius norm of a matrix. Given $N$ examples $\{(\mathbf{x}^i,y^i)|i=1,...,N\}$, we define $\mathbb{E}\{.\}$ as an expectation taken with respect to the empirical distribution generated by the training set.

Following \cite{low-shot}, we aim to learn a neural network model to extract the  feature representations $\phi(\mathbf{x}^i)$ and make predictions $\hat{y}_i=W\phi(\mathbf{x}^i)$ with $W=[\mathbf{w}_1,...,\mathbf{w}_{|C|}]$. This setting includes both classification and regression problems, with $|C|$ as the number of classes or target dimension, respectively. The problem can be generally formulated as:
\begin{equation}
W^*,\phi^*=\arg\min_{W,\phi}\mathbb{E}\{l(W,\phi(\mathbf{x}^i),y^i)\}
\end{equation}
where $l(.)$ can be any reasonable cost function. {\color{red}In this paper, we focus on cross-entropy and  $L_2$ loss due to their convexity and universality.} % Specifically, if $l(.)$ a metric learning loss as in \cite{one-shot-siamese,low-shot}, $W$ is empty and we only optimize $\phi(.)$.% This can be generally optimized with an online stochastic gradient descent (SGD).

In \cite{low-shot}, it is suggested that adding a squared gradient magnitude loss (SGM) on every sample can regularize the learning process.
\begin{equation}
W^*,\phi^*=\arg\min_{W,\phi}\mathbb{E}\{l(W,\phi(\mathbf{x}^i),y^i)+\lambda||\nabla_Wl(W,\phi(\mathbf{x}^i,y^i))||^2\}
\end{equation}
The insight is that for a good solution, the parameter gradient should be small at convergence. However, we know that the convergence of a neural network optimization is a dynamic equilibrium. In other words, at a stationary point, we should have $\mathbb{E}\{\nabla_W l(W,\phi(\mathbf{x}))\}\rightarrow0$. Intuitively %As pointed out in \cite{big-data}, 
when close to convergence, about half of the data-cases recommend to update a parameter to move {\color{red}positive}, while the other half recommend to move {\color{red}negative}. It is not very clear why \textit{small gradients on every sample} $\mathbb{E}\{||\nabla_W l(W,\phi(\mathbf{x}))||^2\}$ produces good generalization experimentally.

Mathematical derivation shows that the optimization problem with gradient penalty is equivalent with adding a weighted $L_2$ regularizer $\phi(\mathbf{x}^i)$:
\begin{eqnarray}
\arg\min_{W,\phi}\mathbb{E}\{l(W,\phi(\mathbf{x}^i),y^i)+\lambda\alpha^i||\phi(\mathbf{x}^i)||^2\}
\label{eqn:cost}
\end{eqnarray}
where the example-dependent $\alpha^i$ measures the deviation between the prediction $\hat{y}^i$ and the target $y^i$.  %$\alpha^i$ is higher for examples that are misclassified, and nears zero for correct ones. 
In a regression problem, we have $\alpha^i=r^2=||\hat{y}^i-y^i||^2$, with the residual $r=\hat{y}^i-y^i$; in a classification problem, we have $\alpha^i=\sum_k(p^i_k-I(y^i=k))^2$. Intuitively, the misclassified high-norm examples might be outliers, and in a low-shot learning scenario, such outliers can pull the learned weight vectors far away from the right solution. In \cite{low-shot}, the authors compare dropping $\alpha^i$ and directly penalizing $||\phi(x^i)||^2$, which performs almost equally well. %experimentally. 

In our work, we argue that a better and more reasonable design should be:
\begin{eqnarray}
\arg\min_{W,\phi}\mathbb{E}\{l(W,\phi(\mathbf{x}^i),y^i)+\lambda_1\alpha^i||\phi(\mathbf{x}^i)||^2\}+\lambda_2||W||_F^2
\label{eqn:final-cost}
\end{eqnarray}
where we add another weight regularizer $||W||_F^2$, which is necessary to avoid degenerate solutions. We will give further explanation in our second case study. In following analysis, we denote the cost in Eqn(\ref{eqn:final-cost}) with example-dependent $\alpha^i$ as \textit{\textbf{weighted $L_2$ feature penalty}}, and the example-independent (setting $\alpha^i\equiv1$) as \textit{\textbf{uniform $L_2$ feature penalty}}. % We will analyze in the following section (case study 2) why the final weight $||W||_F^2$ is necessary.

% However, the $L_1$ regularization achieves inferior performance.
We carry out two case studies: (1) an XOR classification and (2) a regression problem, both empirically and theoretically to analyze how the uniform and weighted $L_2$ feature penalty regularize the neural network. In our paper, we will focus on the \textit{\textbf{uniform}} feature regularization and will also cover the \textit{\textbf{weighted}} scenario as well.

\input{case-study1}
\input{case-study2}
\input{case-study2-2}
\input{batch-norm}
\input{low-shot}
