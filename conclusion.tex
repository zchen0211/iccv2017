\section{Conclusion}
\label{sec:conclusion}
In this work, we conduct an analysis, both empirically and theoretically, on how feature regularization influences and improves low-shot learning performance. By exploiting an XOR	classification and two-layer linear regression, we find that the regularization of feature centers the representation, which in turn makes the learning problem easier with better numerical behavior. From the Bayesian point of view, the feature regularization is closely related to batch normalization. Evaluation on synthetic, ``Omniglot'' one-shot and large-scale ImageNet benchmark validates our analysis.

% One of future directions is to further study and compare what is the best distribution model of the feature representation. Also, how to regularize an RNN to generalize well in low-shot scenario requires more investigation.