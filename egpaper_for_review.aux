\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{alexnet}
\citation{residual_net}
\citation{deep_asr}
\citation{lake-tutorial}
\citation{lake-omniglot}
\citation{learn-to-learn1}
\citation{meta-learn-theory}
\citation{meta-learn-theory}
\citation{feature-transfer}
\citation{low-shot}
\citation{low-shot}
\citation{batch-normalization}
\citation{learn-to-learn1}
\citation{nn-pr}
\citation{low-shot}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{brf}{\backcite{alexnet,residual_net}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{deep_asr}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{lake-tutorial}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{lake-omniglot}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{learn-to-learn1,meta-learn-theory}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{meta-learn-theory}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{feature-transfer}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{low-shot}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{low-shot}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{batch-normalization}{{1}{1}{section.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\hskip -1em.\nobreakspace  {}Related Work}{1}{subsection.1.1}}
\newlabel{sec:related-work}{{1.1}{1}{\hskip -1em.~Related Work}{subsection.1.1}{}}
\@writefile{brf}{\backcite{learn-to-learn1}{{1}{1.1}{subsection.1.1}}}
\citation{dropout}
\citation{batch-normalization}
\citation{residual_net}
\citation{decorr}
\citation{multiverse}
\citation{metric-feature-transfer}
\citation{one-shot-siamese}
\citation{nca}
\citation{one-shot-face}
\citation{mann}
\citation{matching-network}
\citation{low-shot}
\citation{low-shot}
\citation{low-shot}
\@writefile{brf}{\backcite{nn-pr}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{low-shot}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{dropout}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{batch-normalization}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{residual_net}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{decorr}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{multiverse}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{metric-feature-transfer,one-shot-siamese,nca,one-shot-face}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{mann}{{2}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{matching-network}{{2}{1.1}{subsection.1.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}\hskip -1em.\nobreakspace  {}Contributions}{2}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}An Analysis of Feature Regularization}{2}{section.2}}
\@writefile{brf}{\backcite{low-shot}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{low-shot}{{2}{2}{equation.2.1}}}
\newlabel{eqn:cost}{{3}{2}{\hskip -1em.~An Analysis of Feature Regularization}{equation.2.3}{}}
\@writefile{brf}{\backcite{low-shot}{{2}{2}{equation.2.3}}}
\newlabel{eqn:final-cost}{{4}{2}{\hskip -1em.~An Analysis of Feature Regularization}{equation.2.4}{}}
\citation{nn-global-minima}
\citation{deep-global-minima}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Case Study 1: an Empirical Analysis of XOR Classification}{3}{subsection.2.1}}
\newlabel{sec:empirical}{{2.1}{3}{\hskip -1em.~Case Study 1: an Empirical Analysis of XOR Classification}{subsection.2.1}{}}
\newlabel{eqn:xor-reg}{{5}{3}{\hskip -1em.~Case Study 1: an Empirical Analysis of XOR Classification}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Case 1: an empirical study of the XOR classification task. {\bf  The left figure:} the network structure we use: $h_1$ is a linear transformation, $h_2$ is a non-linear transform of $h_1$ and $y$ is the prediction; {\bf  The right column:} The linear transformation maps $\mathbf  {x}$ to $\mathbf  {h_1}$. As shown in the red arrow, an $L_2$ norm penalty on $h_2$ centers the feature of $h_1$ and make the points from different sets separable. 'X's refer to positive examples, and 'O's are negative ones.}}{3}{figure.1}}
\newlabel{fig-xor}{{1}{3}{Case 1: an empirical study of the XOR classification task. {\bf The left figure:} the network structure we use: $h_1$ is a linear transformation, $h_2$ is a non-linear transform of $h_1$ and $y$ is the prediction; {\bf The right column:} The linear transformation maps $\mathbf {x}$ to $\mathbf {h_1}$. As shown in the red arrow, an $L_2$ norm penalty on $h_2$ centers the feature of $h_1$ and make the points from different sets separable. 'X's refer to positive examples, and 'O's are negative ones}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Case 2: a comprehensive study of a two-layer linear neural network for regression task. We minimize the $L_2$ distance between the prediction $\mathaccentV {hat}05E{y}=W_2(W_1\mathbf  {x}+\mathbf  {b}_1)+\mathbf  {b}_2$ and $y$. The latent representation $\mathbf  {h}=W_1\mathbf  {x}+b_1$ is a linear mapping.}}{3}{figure.2}}
\newlabel{fig-regression}{{2}{3}{Case 2: a comprehensive study of a two-layer linear neural network for regression task. We minimize the $L_2$ distance between the prediction $\hat {y}=W_2(W_1\mathbf {x}+\mathbf {b}_1)+\mathbf {b}_2$ and $y$. The latent representation $\mathbf {h}=W_1\mathbf {x}+b_1$ is a linear mapping}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Case Study 2: a Comprehensive Analysis on a Regression Problem}{3}{subsection.2.2}}
\@writefile{brf}{\backcite{nn-global-minima,deep-global-minima}{{3}{2.2}{Item.7}}}
\newlabel{eqn:equal-energy}{{9}{4}{\hskip -1em.~Case Study 2: a Comprehensive Analysis on a Regression Problem}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}$L_2$ Feature Regularization Makes Optimization Easier}{4}{subsubsection.2.2.1}}
\newlabel{eqn:final-regression}{{11}{4}{$L_2$ Feature Regularization Makes Optimization Easier}{equation.2.11}{}}
\newlabel{eqn:center}{{12}{4}{$L_2$ Feature Regularization Makes Optimization Easier}{equation.2.12}{}}
\citation{batch-normalization}
\citation{analysis-loglinear}
\citation{low-shot}
\citation{vc-dim}
\citation{vc-dim-nn}
\citation{low-shot}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Uniform $L_2$ Feature Penalty is a Soft Batch Normalization}{5}{subsection.2.3}}
\newlabel{sec:theoretical}{{2.3}{5}{\hskip -1em.~Uniform $L_2$ Feature Penalty is a Soft Batch Normalization}{subsection.2.3}{}}
\@writefile{brf}{\backcite{batch-normalization}{{5}{2.3}{subsection.2.3}}}
\newlabel{eqn:prob}{{13}{5}{\hskip -1em.~Uniform $L_2$ Feature Penalty is a Soft Batch Normalization}{equation.2.13}{}}
\@writefile{brf}{\backcite{analysis-loglinear}{{5}{2.3}{equation.2.14}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\hskip -1em.\nobreakspace  {}Feature Regularization May Improve Generalization}{5}{subsection.2.4}}
\@writefile{brf}{\backcite{low-shot}{{5}{2.3}{subsection.2.4}}}
\@writefile{brf}{\backcite{vc-dim}{{5}{2.3}{subsection.2.4}}}
\newlabel{eqn:bound}{{15}{5}{\hskip -1em.~Uniform $L_2$ Feature Penalty is a Soft Batch Normalization}{equation.2.15}{}}
\@writefile{brf}{\backcite{vc-dim-nn}{{5}{2.3}{equation.2.15}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Experimental Results}{5}{section.3}}
\newlabel{sec:exp}{{3}{5}{\hskip -1em.~Experimental Results}{section.3}{}}
\citation{adam}
\citation{low-shot}
\citation{lake-omniglot}
\citation{matching-network}
\citation{batch-normalization}
\citation{low-shot}
\citation{matching-network}
\citation{mann}
\citation{low-shot}
\citation{matching-network}
\citation{lake-omniglot}
\citation{lake-omniglot}
\citation{russakovsky2015imagenet}
\citation{residual_net}
\citation{low-shot}
\citation{russakovsky2015imagenet}
\citation{russakovsky2015imagenet}
\citation{batch-normalization}
\citation{batch-normalization}
\citation{batch-normalization}
\citation{residual_net}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Evaluation on the XOR classification task. The red, blue and green lines stand for the accuracy of the Neural Network without any regularization, only the $L_2$ feature penalty, and our model with both weight and feature regularizer, respectively.}}{6}{figure.3}}
\newlabel{fig-xor-plot}{{3}{6}{Evaluation on the XOR classification task. The red, blue and green lines stand for the accuracy of the Neural Network without any regularization, only the $L_2$ feature penalty, and our model with both weight and feature regularizer, respectively}{figure.3}{}}
\@writefile{brf}{\backcite{low-shot}{{6}{3}{section.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Synthetic Datasets}{6}{subsection.3.1}}
\@writefile{brf}{\backcite{adam}{{6}{3.1}{figure.3}}}
\@writefile{brf}{\backcite{low-shot}{{6}{3.1}{figure.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Low-shot learning on Omniglot}{6}{subsection.3.2}}
\@writefile{brf}{\backcite{lake-omniglot}{{6}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{matching-network}{{6}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{batch-normalization}{{6}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{low-shot}{{6}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{mann}{{6}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{low-shot}{{6}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{matching-network}{{6}{3.2}{subsection.3.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Experimental results of our algorithm on the Omniglot benchmark \cite  {lake-omniglot}. }}{6}{table.1}}
\@writefile{brf}{\backcite{lake-omniglot}{{6}{1}{table.1}}}
\newlabel{tab:omniglot}{{3.2}{6}{\hskip -1em.~Low-shot learning on Omniglot}{table.1}{}}
\@writefile{brf}{\backcite{low-shot}{{6}{3.3}{subsection.3.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Experimental results of our algorithm on the ImageNet benchmark \cite  {russakovsky2015imagenet} with the 20-way one-shot setting. }}{6}{table.2}}
\@writefile{brf}{\backcite{russakovsky2015imagenet}{{6}{2}{table.2}}}
\newlabel{tab:imagenet}{{3.3}{6}{\hskip -1em.~Large-scale Low-shot Learning on ImageNet}{table.2}{}}
\@writefile{brf}{\backcite{matching-network}{{6}{3.2}{subsection.3.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Large-scale Low-shot Learning on ImageNet}{6}{subsection.3.3}}
\@writefile{brf}{\backcite{russakovsky2015imagenet}{{6}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{residual_net}{{6}{3.3}{subsection.3.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Comparison with Batch-Normalization}{6}{subsection.3.4}}
\@writefile{brf}{\backcite{batch-normalization}{{6}{3.3}{figure.4}}}
\@writefile{brf}{\backcite{residual_net}{{6}{3.3}{figure.4}}}
\bibstyle{ieee}
\bibdata{iclr2017_conference}
\bibcite{nn-global-minima}{1}
\bibcite{feature-transfer}{2}
\bibcite{meta-learn-theory}{3}
\bibcite{nn-pr}{4}
\bibcite{one-shot-face}{5}
\bibcite{decorr}{6}
\bibcite{metric-feature-transfer}{7}
\bibcite{nca}{8}
\bibcite{low-shot}{9}
\bibcite{residual_net}{10}
\bibcite{batch-normalization}{11}
\bibcite{deep-global-minima}{12}
\bibcite{adam}{13}
\bibcite{one-shot-siamese}{14}
\bibcite{alexnet}{15}
\bibcite{lake-tutorial}{16}
\bibcite{lake-omniglot}{17}
\bibcite{multiverse}{18}
\bibcite{russakovsky2015imagenet}{19}
\bibcite{mann}{20}
\bibcite{vc-dim-nn}{21}
\bibcite{dropout}{22}
\bibcite{learn-to-learn1}{23}
\bibcite{vc-dim}{24}
\bibcite{matching-network}{25}
\bibcite{analysis-loglinear}{26}
\bibcite{deep_asr}{27}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\color  {red}Classification accuracy comparison of our feature penalty (termed ``PN'') with batch-normalization (termed ``BN'') \cite  {batch-normalization} on MNIST, CIFAR-10, Omniglot and ImageNet benchmarks. We compare baseline methods with neither batch-normalization nor feature penalty, with each module added, and with modules included.}}}{7}{figure.4}}
\@writefile{brf}{\backcite{batch-normalization}{{7}{4}{figure.4}}}
\newlabel{fig-compare-bn}{{4}{7}{\color {red}Classification accuracy comparison of our feature penalty (termed ``PN'') with batch-normalization (termed ``BN'') \cite {batch-normalization} on MNIST, CIFAR-10, Omniglot and ImageNet benchmarks. We compare baseline methods with neither batch-normalization nor feature penalty, with each module added, and with modules included}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Conclusion}{7}{section.4}}
\newlabel{sec:conclusion}{{4}{7}{\hskip -1em.~Conclusion}{section.4}{}}
